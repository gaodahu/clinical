{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D4E-Oyc6eaw"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def parse_xml(xml_file):\n",
        "    # Parse XML with ElementTree\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    text_list = []\n",
        "\n",
        "    # Iterate through each element in the XML\n",
        "    for elem in root.iter('String'):\n",
        "        text_list.append(elem.text)\n",
        "\n",
        "    return text_list\n",
        "\n",
        "qual2023 = '/content/drive/MyDrive/xml/qual2023.xml'\n",
        "pa2023 = '/content/drive/MyDrive/xml/pa2023.xml'\n",
        "desc2023 = '/content/drive/MyDrive/xml/desc2023.xml'\n",
        "supp2023 = '/content/drive/MyDrive/xml/supp2023.xml'\n",
        "qual2023 = parse_xml(qual2023)\n",
        "pa2023 = parse_xml(pa2023)\n",
        "desc2023 = parse_xml(desc2023)\n",
        "supp2023 = parse_xml(supp2023)\n",
        "# print(qual2023)\n",
        "# print(pa2023)\n",
        "# print(desc2023)\n",
        "# print(qual2023)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWaUI35tNOuB",
        "outputId": "d33aef8d-ba2d-4f06-bb18-f76ed762a2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined = qual2023 + pa2023 + desc2023 + qual2023\n",
        "\n",
        "# Convert to a set to remove duplicates, then convert back to a list\n",
        "unique_words = list(set(combined))\n",
        "\n",
        "# print(unique_words[:100])"
      ],
      "metadata": {
        "id": "-XnqZiGpORHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from the text file\n",
        "drug_path='/content/drive/MyDrive/drug/Products.txt'\n",
        "\n",
        "data = pd.read_csv(drug_path, delimiter='\\t', error_bad_lines=False)\n",
        "\n",
        "# Get the drug names\n",
        "drug_names = data['DrugName'].tolist()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMj7XerIMMJP",
        "outputId": "c208bfb0-49a7-43c6-c635-412e1a35aaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1706ac7f50ff>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data = pd.read_csv(drug_path, delimiter='\\t', error_bad_lines=False)\n",
            "Skipping line 35431: expected 8 fields, saw 9\n",
            "Skipping line 35432: expected 8 fields, saw 9\n",
            "Skipping line 35433: expected 8 fields, saw 9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the drug names\n",
        "unique_drugs = list(set(drug_names))\n",
        "\n",
        "# print(len(unique_drugs))"
      ],
      "metadata": {
        "id": "n994jQUVNPjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined = unique_drugs + unique_words\n",
        "\n",
        "# Convert to a set to remove duplicates, then convert back to a list\n",
        "vocabulary = list(set(combined))\n",
        "\n",
        "# print(vocabulary[:100])"
      ],
      "metadata": {
        "id": "SMSXhigTNZV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/vocabulary.txt'\n",
        "with open(path, \"w\") as file:\n",
        "    for word in vocabulary:\n",
        "        file.write(f\"{word}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zxkg3TV5Rc4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word to search\n",
        "search_word = 'bromocriptine'\n",
        "\n",
        "# Check if the word is in the list\n",
        "\n",
        "for phrase in unique_words:\n",
        "    # Convert both the search word and phrase to lower case for case-insensitive comparison\n",
        "    if search_word.lower() == phrase.lower():\n",
        "\n",
        "        print(f\"The search word '{search_word}' is found in the disease name '{phrase}'\")\n",
        "\n",
        "for phrase in unique_drugs:\n",
        "    # Convert both the search word and phrase to lower case for case-insensitive comparison\n",
        "    if search_word.lower() == phrase.lower():\n",
        "\n",
        "        print(f\"The search word '{search_word}' is found in the drug name '{phrase}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPQs1se8OC1P",
        "outputId": "444fadea-4da4-403d-b41d-e0991dd80a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The search word 'bromocriptine' is found in the disease name 'Bromocriptine'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(r'/content/drive/MyDrive/bart/MeQSum_ACL2019_BenAbacha_Demner-Fushman.xlsx')\n",
        "text_list = data['CHQ'].tolist()"
      ],
      "metadata": {
        "id": "KHuwU7tYMVRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def get_all_ngrams(text, n):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    all_ngrams = []\n",
        "    for i in range(1, n+1):\n",
        "        all_ngrams.extend([' '.join(ngram) for ngram in ngrams(tokens, i)])\n",
        "    return all_ngrams\n",
        "\n",
        "\n",
        "def get_vocab_in_text(text, vocab):\n",
        "    vocab = [item.lower() for item in vocab]\n",
        "    max_n = max(len(word_tokenize(item)) for item in vocab)\n",
        "    text_ngrams = get_all_ngrams(text, max_n)\n",
        "\n",
        "    vocab_in_text = [item for item in text_ngrams if item in vocab]\n",
        "    return vocab_in_text"
      ],
      "metadata": {
        "id": "jDS4oLcLMjep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVQP-9IdSBJV",
        "outputId": "85733b8b-4a3a-458c-d0ee-d1083b720bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity=[]\n",
        "for text in text_list:\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  text_bigrams_1 = [' '.join(ngram) for ngram in ngrams(tokens, 1)]\n",
        "  text_bigrams_2 = [' '.join(ngram) for ngram in ngrams(tokens, 2)]\n",
        "  text_bigrams_3 = [' '.join(ngram) for ngram in ngrams(tokens, 3)]\n",
        "\n",
        "  matching_phrases_1 = [phrase for phrase in vocabulary if phrase in text_bigrams_1]\n",
        "  matching_phrases_2 = [phrase for phrase in vocabulary if phrase in text_bigrams_2]\n",
        "  matching_phrases_3 = [phrase for phrase in vocabulary if phrase in text_bigrams_3]\n",
        "  combined = matching_phrases_1 + matching_phrases_2+matching_phrases_3\n",
        "  entity.append(combined)\n",
        "print(entity)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-J6ZnPiU6mQR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "73d05ce9-26e2-4c28-8475-f1c89d247af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-edc42f339fd5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Now we can look for our vocabulary phrases in the list of bigrams:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mmatching_phrases_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_bigrams_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mmatching_phrases_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_bigrams_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmatching_phrases_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_bigrams_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-edc42f339fd5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Now we can look for our vocabulary phrases in the list of bigrams:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mmatching_phrases_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_bigrams_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mmatching_phrases_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_bigrams_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmatching_phrases_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mphrase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_bigrams_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "disease=[]\n",
        "drugs=[]\n",
        "\n",
        "for text in text_list[0:10]:\n",
        "  # tokens = word_tokenize(text.lower())\n",
        "\n",
        "  # text_bigrams_1 = [' '.join(ngram) for ngram in ngrams(tokens, 1)]\n",
        "  # text_bigrams_2 = [' '.join(ngram) for ngram in ngrams(tokens, 2)]\n",
        "  # text_bigrams_3 = [' '.join(ngram) for ngram in ngrams(tokens, 3)]\n",
        "\n",
        "  # matching_phrases_1_disease = [phrase.lower() for phrase in unique_words if phrase.lower() in text_bigrams_1]\n",
        "  # matching_phrases_2_disease = [phrase.lower() for phrase in unique_words if phrase.lower() in text_bigrams_2]\n",
        "  # matching_phrases_3_disease = [phrase.lower() for phrase in unique_words if phrase.lower() in text_bigrams_3]\n",
        "  # matching_phrases_1_drug = [phrase.lower() for phrase in unique_drugs if phrase.lower() in text_bigrams_1]\n",
        "  # matching_phrases_2_drug = [phrase.lower() for phrase in unique_drugs if phrase.lower() in text_bigrams_2]\n",
        "  # matching_phrases_3_drug = [phrase.lower() for phrase in unique_drugs if phrase.lower() in text_bigrams_3]\n",
        "\n",
        "  # combined_disease = matching_phrases_1_disease + matching_phrases_2_disease+matching_phrases_3_disease\n",
        "  # disease.append(combined_disease)\n",
        "  # combined_drugs = matching_phrases_1_drug + matching_phrases_2_drug+matching_phrases_3_drug\n",
        "  # drugs.append(combined_drugs)\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  joined_tokens = ' '.join(tokens)\n",
        "\n",
        "  matching_phrases_disease = [phrase.lower() for phrase in unique_words if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', joined_tokens)]\n",
        "  matching_phrases_drug = [phrase.lower() for phrase in unique_drugs if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', joined_tokens)]\n",
        "\n",
        "  disease.append(matching_phrases_disease)\n",
        "  drugs.append(matching_phrases_drug)\n",
        "print(\"Drugs:\",drugs)\n",
        "print(\"Disease:\",disease)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULdjpHn9S9A4",
        "outputId": "7f9e8fa7-9e7a-457f-ed80-0a63f004d263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drugs: [[], [], ['nulytely'], [], [], [], [], ['today'], [], []]\n",
            "Disease: [['cetirizine', 'supply', 'who'], ['costs', 'price', 'drug', 'pituitary gland', 'cost', 'who', 'bromocriptine', 'cost', 'medicine', 'prescription'], ['who'], ['daughter', 'who', 'syndrome'], ['multiple myeloma', 'female', 'parents', 'oncologist', 'cost', 'mother', 'health', 'father', 'cost'], ['who', 'family', 'heart', 'texas'], ['treatment', 'parents', 'chromosome', 'treatment', 'ataxia'], ['genetics', 'death', 'diseases', 'muscle', 'recurrence', 'rare diseases', 'military', 'deficiency', 'genetic testing', 'medicine', 'genetics', 'information center', 'texas', 'sister'], ['hip', 'use', 'work', 'stomach', 'time'], ['son', 'panniculitis', 'omental panniculitis', 'disease']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m-DQn6MY8xE",
        "outputId": "9589385f-bf8c-4bfc-bc39-0bd4ad58bf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('/content/drive/MyDrive/txt/disease.txt', 'w') as f:\n",
        "    for item in unique_words:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "QIAF_IFVZgJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cartTQU6y_80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocabulary = [item.lower() for item in vocabulary]\n",
        "item_to_remove = ['supply','who','costs','price','cost','prescription','daughter','female','parents','health','mother','farther','family','texas','today','sister','brother','military','father','son','time','use','work','woman','man','male','lunch','dinner']\n",
        "for item in item_to_remove:\n",
        "  if item in vocabulary:\n",
        "    vocabulary.remove(item)\n",
        "clinical_dict = {\"Clinical Phrase\": vocabulary}\n"
      ],
      "metadata": {
        "id": "3-93wrZBrDnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_search = 'price'\n",
        "\n",
        "if word_to_search.lower() in clinical_dict['Clinical Phrase']:\n",
        "    print(f'{word_to_search} is a value in the dictionary.')\n",
        "else:\n",
        "    print(f'{word_to_search} is not a value in the dictionary.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlL7rvdtw42l",
        "outputId": "6ef74911-4b6b-46af-f02a-7ad6b3c5b561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price is a value in the dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clinical=[]\n",
        "\n",
        "\n",
        "for text in text_list:\n",
        "  tokens = word_tokenize(text.lower())\n",
        "\n",
        "  text_bigrams_1 = [' '.join(ngram) for ngram in ngrams(tokens, 1)]\n",
        "  text_bigrams_2 = [' '.join(ngram) for ngram in ngrams(tokens, 2)]\n",
        "  text_bigrams_3 = [' '.join(ngram) for ngram in ngrams(tokens, 3)]\n",
        "\n",
        "  matching_phrases_1_disease = [phrase.lower() for phrase in clinical_dict['Clinical Phrase'] if phrase.lower() in text_bigrams_1]\n",
        "  matching_phrases_2_disease = [phrase.lower() for phrase in clinical_dict['Clinical Phrase'] if phrase.lower() in text_bigrams_2]\n",
        "  matching_phrases_3_disease = [phrase.lower() for phrase in clinical_dict['Clinical Phrase'] if phrase.lower() in text_bigrams_3]\n",
        "  matching_phrases_1_drug = [phrase.lower() for phrase in clinical_dict['Clinical Phrase'] if phrase.lower() in text_bigrams_1]\n",
        "  matching_phrases_2_drug = [phrase.lower() for phrase in clinical_dict['Clinical Phrase'] if phrase.lower() in text_bigrams_2]\n",
        "  matching_phrases_3_drug = [phrase.lower() for phrase in clinical_dict['Clinical Phrase'] if phrase.lower() in text_bigrams_3]\n",
        "\n",
        "  combined_disease = matching_phrases_1_disease + matching_phrases_2_disease+matching_phrases_3_disease\n",
        "  clinical.append(combined_disease)\n",
        "\n",
        "  # tokens = word_tokenize(text.lower())\n",
        "  # joined_tokens = ' '.join(tokens)\n",
        "\n",
        "  # matching_phrases_disease = [phrase.lower() for phrase in unique_words if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', joined_tokens)]\n",
        "  # matching_phrases_drug = [phrase.lower() for phrase in unique_drugs if re.search(r'\\b' + re.escape(phrase.lower()) + r'\\b', joined_tokens)]\n",
        "\n",
        "  # disease.append(matching_phrases_disease)\n",
        "  # drugs.append(matching_phrases_drug)\n",
        "print(\"clinical:\",clinical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j49X18K9xSFd",
        "outputId": "fcbf031b-48bf-487b-826c-21edb7acde06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clinical: [['lunch', 'eating', 'stomach', 'pain', 'abdominal pain'], ['ligation', 'pregnancy', 'tubal ligation'], ['thyroid', 'surgery', 'cancer', 'surgery', 'thyroid cancer'], ['temperatures', 'thought', 'hope', 'lisinopril', 'lisinopril', 'will', 'let', 'drugs', 'digoxin', 'digoxin'], ['skin', 'neck', 'color', 'device', 'arms'], ['interview', 'cribs', 'death', 'research', 'learning', 'student', 'email', 'resources', 'report', 'sids', 'name', 'weather', 'research report'], ['organization', 'medlineplus', 'diagnose', 'association', 'disease', 'resources', 'organization'], ['medicine'], ['prolapse', 'rectum', 'address', 'rectal prolapse']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtSm1v4mTURo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJPfJxwdTUnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}